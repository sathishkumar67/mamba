{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://huggingface.co/pt-sk/m","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:30:12.489624Z","iopub.execute_input":"2024-06-06T06:30:12.489891Z","iopub.status.idle":"2024-06-06T06:30:57.359776Z","shell.execute_reply.started":"2024-06-06T06:30:12.489867Z","shell.execute_reply":"2024-06-06T06:30:57.358812Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'm'...\nremote: Enumerating objects: 32, done.\u001b[K\nremote: Counting objects: 100% (28/28), done.\u001b[K\nremote: Compressing objects: 100% (28/28), done.\u001b[K\nremote: Total 32 (delta 10), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\nUnpacking objects: 100% (32/32), 11.74 KiB | 1.47 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install einops\n!pip install trl\n!pip install transformers\n!pip install transformers[torch]\n!pip install accelerate -U\n!pip install fairscale","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:30:57.361991Z","iopub.execute_input":"2024-06-06T06:30:57.362634Z","iopub.status.idle":"2024-06-06T06:32:54.962312Z","shell.execute_reply.started":"2024-06-06T06:30:57.362594Z","shell.execute_reply":"2024-06-06T06:32:54.961216Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m627.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting trl\n  Downloading trl-0.9.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.41.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.30.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.19.2)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.3.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.23.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.9.3-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.6/226.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.7.1 trl-0.9.3 tyro-0.8.4\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.30.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting fairscale\n  Downloading fairscale-0.4.13.tar.gz (266 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from fairscale) (2.1.2)\nRequirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from fairscale) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->fairscale) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->fairscale) (1.3.0)\nBuilding wheels for collected packages: fairscale\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332108 sha256=230be20097f4cf8f253afe4c445afcbf680b54c95f3d4827ee2b74bae4d47861\n  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\nSuccessfully built fairscale\nInstalling collected packages: fairscale\nSuccessfully installed fairscale-0.4.13\n","output_type":"stream"}]},{"cell_type":"code","source":"# necessary libraries\nfrom __future__ import annotations\nimport math\nimport json\nimport io\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\n\nimport datasets\nfrom dataclasses import dataclass\nfrom datasets import load_dataset, Dataset\nfrom einops import rearrange, repeat, einsum\nfrom typing import Union\n\nfrom transformers import AutoTokenizer\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport fairscale\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\")\n\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\n# Device initialization\n# device = xm.xla_device()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:32:54.963679Z","iopub.execute_input":"2024-06-06T06:32:54.963985Z","iopub.status.idle":"2024-06-06T06:33:01.138249Z","shell.execute_reply.started":"2024-06-06T06:32:54.963959Z","shell.execute_reply":"2024-06-06T06:33:01.137510Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\nAn implementation of the parallel scan operation in PyTorch (Blelloch version).\nPlease see docs/pscan.ipynb for a detailed explanation of what happens here.\n\n\"\"\"\n\ndef npo2(len):\n    \"\"\"\n    Returns the next power of 2 above len\n    \"\"\"\n\n    return 2 ** math.ceil(math.log2(len))\n\ndef pad_npo2(X):\n    \"\"\"\n    Pads input length dim to the next power of 2\n\n    Args:\n        X : (B, L, D, N)\n\n    Returns:\n        Y : (B, npo2(L), D, N)\n    \"\"\"\n\n    len_npo2 = npo2(X.size(1))\n    pad_tuple = (0, 0, 0, 0, 0, len_npo2 - X.size(1))\n    return F.pad(X, pad_tuple, \"constant\", 0)\n\nclass PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n\n        # only supports L that is a power of two (mainly for a clearer code)\n        \n        B, D, L, _ = A.size()\n        num_steps = int(math.log2(L))\n\n        # up sweep (last 2 steps unfolded)\n        Aa = A\n        Xa = X\n        for _ in range(num_steps-2):\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T//2, 2, -1)\n            Xa = Xa.view(B, D, T//2, 2, -1)\n            \n            Xa[:, :, :, 1].add_(Aa[:, :, :, 1].mul(Xa[:, :, :, 0]))\n            Aa[:, :, :, 1].mul_(Aa[:, :, :, 0])\n\n            Aa = Aa[:, :, :, 1]\n            Xa = Xa[:, :, :, 1]\n\n        # we have only 4, 2 or 1 nodes left\n        if Xa.size(2) == 4:\n            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n            Aa[:, :, 1].mul_(Aa[:, :, 0])\n\n            Xa[:, :, 3].add_(Aa[:, :, 3].mul(Xa[:, :, 2] + Aa[:, :, 2].mul(Xa[:, :, 1])))\n        elif Xa.size(2) == 2:\n            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n            return\n        else:\n            return\n\n        # down sweep (first 2 steps unfolded)\n        Aa = A[:, :, 2**(num_steps-2)-1:L:2**(num_steps-2)]\n        Xa = X[:, :, 2**(num_steps-2)-1:L:2**(num_steps-2)]\n        Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 1]))\n        Aa[:, :, 2].mul_(Aa[:, :, 1])\n\n        for k in range(num_steps-3, -1, -1):\n            Aa = A[:, :, 2**k-1:L:2**k]\n            Xa = X[:, :, 2**k-1:L:2**k]\n\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T//2, 2, -1)\n            Xa = Xa.view(B, D, T//2, 2, -1)\n\n            Xa[:, :, 1:, 0].add_(Aa[:, :, 1:, 0].mul(Xa[:, :, :-1, 1]))\n            Aa[:, :, 1:, 0].mul_(Aa[:, :, :-1, 1])\n\n    @staticmethod\n    def pscan_rev(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n\n        # the same function as above, but in reverse\n        # (if you flip the input, call pscan, then flip the output, you get what this function outputs)\n        # it is used in the backward pass\n\n        # only supports L that is a power of two (mainly for a clearer code)\n\n        B, D, L, _ = A.size()\n        num_steps = int(math.log2(L))\n\n        # up sweep (last 2 steps unfolded)\n        Aa = A\n        Xa = X\n        for _ in range(num_steps-2):\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T//2, 2, -1)\n            Xa = Xa.view(B, D, T//2, 2, -1)\n                    \n            Xa[:, :, :, 0].add_(Aa[:, :, :, 0].mul(Xa[:, :, :, 1]))\n            Aa[:, :, :, 0].mul_(Aa[:, :, :, 1])\n\n            Aa = Aa[:, :, :, 0]\n            Xa = Xa[:, :, :, 0]\n\n        # we have only 4, 2 or 1 nodes left\n        if Xa.size(2) == 4:\n            Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 3]))\n            Aa[:, :, 2].mul_(Aa[:, :, 3])\n\n            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1].add(Aa[:, :, 1].mul(Xa[:, :, 2]))))\n        elif Xa.size(2) == 2:\n            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1]))\n            return\n        else:\n            return\n\n        # down sweep (first 2 steps unfolded)\n        Aa = A[:, :, 0:L:2**(num_steps-2)]\n        Xa = X[:, :, 0:L:2**(num_steps-2)]\n        Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 2]))\n        Aa[:, :, 1].mul_(Aa[:, :, 2])\n\n        for k in range(num_steps-3, -1, -1):\n            Aa = A[:, :, 0:L:2**k]\n            Xa = X[:, :, 0:L:2**k]\n\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T//2, 2, -1)\n            Xa = Xa.view(B, D, T//2, 2, -1)\n\n            Xa[:, :, :-1, 1].add_(Aa[:, :, :-1, 1].mul(Xa[:, :, 1:, 0]))\n            Aa[:, :, :-1, 1].mul_(Aa[:, :, 1:, 0])\n\n    @staticmethod\n    def forward(ctx, A_in, X_in):\n        \"\"\"\n        Applies the parallel scan operation, as defined above. Returns a new tensor.\n        If you can, privilege sequence lengths that are powers of two.\n\n        Args:\n            A_in : (B, L, D, N)\n            X_in : (B, L, D, N)\n\n        Returns:\n            H : (B, L, D, N)\n        \"\"\"\n\n        L = X_in.size(1)\n\n        # cloning is requiered because of the in-place ops\n        if L == npo2(L):\n            A = A_in.clone()\n            X = X_in.clone()\n        else:\n            # pad tensors (and clone btw)\n            A = pad_npo2(A_in) # (B, npo2(L), D, N)\n            X = pad_npo2(X_in) # (B, npo2(L), D, N)\n        \n        # prepare tensors\n        A = A.transpose(2, 1) # (B, D, npo2(L), N)\n        X = X.transpose(2, 1) # (B, D, npo2(L), N)\n\n        # parallel scan (modifies X in-place)\n        PScan.pscan(A, X)\n\n        ctx.save_for_backward(A_in, X)\n        \n        # slice [:, :L] (cut if there was padding)\n        return X.transpose(2, 1)[:, :L]\n    \n    @staticmethod\n    def backward(ctx, grad_output_in):\n        \"\"\"\n        Flows the gradient from the output to the input. Returns two new tensors.\n\n        Args:\n            ctx : A_in : (B, L, D, N), X : (B, D, L, N)\n            grad_output_in : (B, L, D, N)\n\n        Returns:\n            gradA : (B, L, D, N), gradX : (B, L, D, N)\n        \"\"\"\n\n        A_in, X = ctx.saved_tensors\n\n        L = grad_output_in.size(1)\n\n        # cloning is requiered because of the in-place ops\n        if L == npo2(L):\n            grad_output = grad_output_in.clone()\n            # the next padding will clone A_in\n        else:\n            grad_output = pad_npo2(grad_output_in) # (B, npo2(L), D, N)\n            A_in = pad_npo2(A_in) # (B, npo2(L), D, N)\n\n        # prepare tensors\n        grad_output = grad_output.transpose(2, 1)\n        A_in = A_in.transpose(2, 1) # (B, D, npo2(L), N)\n        A = torch.nn.functional.pad(A_in[:, :, 1:], (0, 0, 0, 1)) # (B, D, npo2(L), N) shift 1 to the left (see hand derivation)\n\n        # reverse parallel scan (modifies grad_output in-place)\n        PScan.pscan_rev(A, grad_output)\n\n        Q = torch.zeros_like(X)\n        Q[:, :, 1:].add_(X[:, :, :-1] * grad_output[:, :, 1:])\n\n        return Q.transpose(2, 1)[:, :L], grad_output.transpose(2, 1)[:, :L]\n    \npscan = PScan.apply","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:01.141056Z","iopub.execute_input":"2024-06-06T06:33:01.141615Z","iopub.status.idle":"2024-06-06T06:33:01.180565Z","shell.execute_reply.started":"2024-06-06T06:33:01.141582Z","shell.execute_reply":"2024-06-06T06:33:01.179637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"Simple, minimal implementation of Mamba in one file of PyTorch.\n\nSuggest reading the following before/while reading the code:\n    [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao)\n        https://arxiv.org/abs/2312.00752\n    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti)\n        https://srush.github.io/annotated-s4\n\nGlossary:\n    b: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n    l: sequence length                  (`L` in [1] Algorithm 2)\n    d or d_model: hidden dim\n    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n    expand: expansion factor            (`E` in [1] Section 3.4)\n    d_in or d_inner: d * expand         (`D` in [1] Algorithm 2)\n    A, B, C, D: state space parameters  (See any state space representation formula)\n                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n    Δ or delta: input-dependent step size\n    dt_rank: rank of Δ                  (See [1] Section 3.6 \"Parameterization of ∆\")\n\n\"\"\"\n@dataclass\nclass ModelArgs:\n    d_model: int\n    n_layer: int\n    vocab_size: int\n    d_state: int = 16\n    expand: int = 2\n    dt_rank: Union[int, str] = 'auto'\n    d_conv: int = 4 \n    pad_vocab_size_multiple: int = 8\n    conv_bias: bool = True\n    bias: bool = False\n    \n    def __post_init__(self):\n        self.d_inner = int(self.expand * self.d_model)\n        \n        if self.dt_rank == 'auto':\n            self.dt_rank = math.ceil(self.d_model / 16)\n            \n        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n            self.vocab_size += (self.pad_vocab_size_multiple\n                                - self.vocab_size % self.pad_vocab_size_multiple)\n\nclass Mamba(nn.Module):\n    \n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        \n        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n        self.norm_f = RMSNorm(args.d_model)\n\n        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n#         self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n                                                     # See \"Weight Tying\" paper\n\n    def forward(self):\n        \"\"\"\n        Args:\n            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n    \n        Returns:\n            logits: shape (b, l, vocab_size)\n        \"\"\"\n        layers = nn.Sequential(self.embedding, *self.layers, self.norm_f, self.lm_head)\n        return layers\n\n\n    @staticmethod\n    def from_config(pretrained_model_name: str):\n      from transformers.utils import CONFIG_NAME\n      from transformers.utils.hub import cached_file\n      \n      def load_config_hf(model_name):\n          resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n                                              _raise_exceptions_for_missing_entries=False)\n          return json.load(open(resolved_archive_file))\n      config_data = load_config_hf(pretrained_model_name)\n      args = ModelArgs(\n          d_model=config_data['d_model'],\n          n_layer=config_data['n_layer'],\n          vocab_size=config_data['vocab_size']\n      )\n      model = Mamba(args)\n      return model\n\n    \n    @staticmethod\n    def from_pretrained(pretrained_model_name: str):\n        \"\"\"Load pretrained weights from HuggingFace into model.\n    \n        Args:\n            pretrained_model_name: One of\n                * 'state-spaces/mamba-2.8b-slimpj'\n                * 'state-spaces/mamba-2.8b'\n                * 'state-spaces/mamba-1.4b'\n                * 'state-spaces/mamba-790m'\n                * 'state-spaces/mamba-370m'\n                * 'state-spaces/mamba-130m'\n                            \n        Returns:\n            model: Mamba model with weights loaded\n    \n        \"\"\"\n        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n        from transformers.utils.hub import cached_file\n        \n        def load_config_hf(model_name):\n            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n                                                _raise_exceptions_for_missing_entries=False)\n            return json.load(open(resolved_archive_file))\n        \n        \n        def load_state_dict_hf(model_name, device=None, dtype=None):\n            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n                                                _raise_exceptions_for_missing_entries=False)\n            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n        \n        config_data = load_config_hf(pretrained_model_name)\n        args = ModelArgs(\n            d_model=config_data['d_model'],\n            n_layer=config_data['n_layer'],\n            vocab_size=config_data['vocab_size']\n        )\n        model = Mamba(args)\n        \n        state_dict = load_state_dict_hf(pretrained_model_name)\n        new_state_dict = {}\n        for key in state_dict:\n            new_key = key.replace('backbone.', '')\n            new_state_dict[new_key] = state_dict[key]\n        model.load_state_dict(new_state_dict)\n        \n        return model\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n        super().__init__()\n        self.args = args\n        self.mixer = MambaBlock(args)\n        self.norm = RMSNorm(args.d_model)\n        \n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n    \n        Returns:\n            output: shape (b, l, d)\n        \"\"\"\n        output = self.mixer(self.norm(x)) + x\n\n        return output\n            \n\nclass MambaBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n        super().__init__()\n        self.args = args\n\n        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n\n        self.conv1d = nn.Conv1d(\n            in_channels=args.d_inner,\n            out_channels=args.d_inner,\n            bias=args.conv_bias,\n            kernel_size=args.d_conv,\n            groups=args.d_inner,\n            padding=args.d_conv - 1,\n        )\n\n        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n        \n        # dt_proj projects Δ from dt_rank to d_in\n        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n\n        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n        self.A_log = nn.Parameter(torch.log(A))\n        self.D = nn.Parameter(torch.ones(args.d_inner))\n        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n        \n\n    def forward(self, x):\n        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n    \n        Args:\n            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n    \n        Returns:\n            output: shape (b, l, d)  \n        \"\"\"\n        (b, l, d) = x.shape\n        \n        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n\n        x = rearrange(x, 'b l d_in -> b d_in l')\n        x = self.conv1d(x)[:, :, :l]\n        x = rearrange(x, 'b d_in l -> b l d_in')\n        \n        x = F.silu(x)\n\n        y = self.ssm(x)\n        \n        y = y * F.silu(res)\n        \n        output = self.out_proj(y)\n\n        return output\n\n    \n    def ssm(self, x):\n        \"\"\"Runs the SSM. See:\n            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n\n        Args:\n            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n    \n        Returns:\n            output: shape (b, l, d_in)\n\n        Official Implementation:\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n            \n        \"\"\"\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n        #                                  and is why Mamba is called **selective** state spaces)\n        \n        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n        D = self.D.float()\n\n        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n        \n        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n        \n        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n        \n        return y\n\n    \n    def selective_scan(self, x, delta, A, B, C, D):\n        \"\"\"Does selective scan algorithm. See:\n            - Section 2 State Space Models in the Mamba paper [1]\n            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n\n        This is the classic discrete state space formula:\n            x(t + 1) = Ax(t) + Bu(t)\n            y(t)     = Cx(t) + Du(t)\n        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n    \n        Args:\n            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n            delta: shape (b, l, d_in)\n            A: shape (d_in, n)\n            B: shape (b, l, n)\n            C: shape (b, l, n)\n            D: shape (d_in,)\n    \n        Returns:\n            output: shape (b, l, d_in) \n        \"\"\"\n        # parallel scan\n        deltaA = torch.exp(delta.unsqueeze(-1) * A) # (B, L, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) # (B, L, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1)) # (B, L, ED, N)\n        \n        hs = pscan(deltaA, BX)\n\n        y = (hs @ C.unsqueeze(-1)).squeeze(3) # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n\n        y = y + D * x\n\n        return y\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self,\n                 d_model: int,\n                 eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n\n\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n\n        return output        ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:01.181827Z","iopub.execute_input":"2024-06-06T06:33:01.182310Z","iopub.status.idle":"2024-06-06T06:33:01.221323Z","shell.execute_reply.started":"2024-06-06T06:33:01.182264Z","shell.execute_reply":"2024-06-06T06:33:01.220494Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# \"flytech/python-codes-25k\" --- 2000 epochs\n# \"iamtarun/python_code_instructions_18k_alpaca\"----1500 epochs\n# muellerzr/python-stack-v1-functions-filtered-llama-3-8B----8000 epochs\n# bigcode/python-stack-v1-functions-filtered-sc2-subset------2000 epochs\n# jean1/45k_python_code_chinese_instruction ---- 2500 epochs\n# MohamedSaeed-dev/PythonDataV2 ---- 3000 epochs\n# Vezora/Tested-143k-Python-Alpaca --- 4000 epochs -- still yet to train for more epochs\n\n# HydraLM/instruct-python-500k-standardized --- 2000 epochs-----trained for the first 100k\n# \"AayushMathur/manim_python_alpaca\"\n# \"gauravvaid/python-code_samples\"\n# Fraser/python-state-changes\n# \"mengmengmmm/csn_python_trainuse\"","metadata":{"execution":{"iopub.status.busy":"2024-06-05T07:19:45.927564Z","iopub.execute_input":"2024-06-05T07:19:45.928521Z","iopub.status.idle":"2024-06-05T07:19:45.936470Z","shell.execute_reply.started":"2024-06-05T07:19:45.928491Z","shell.execute_reply":"2024-06-05T07:19:45.935490Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ds = load_dataset(\"Vezora/Tested-143k-Python-Alpaca\")\nds","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:01.222650Z","iopub.execute_input":"2024-06-06T06:33:01.223214Z","iopub.status.idle":"2024-06-06T06:33:13.618655Z","shell.execute_reply.started":"2024-06-06T06:33:01.223182Z","shell.execute_reply":"2024-06-06T06:33:13.617756Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/511 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c368d01755d041aaad73acb167052ddb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/164M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959d00da7f764e4da4fbd24449d441b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/180M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5398141668e49b4a73562b57d3454d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/185M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb15ef3f7974f29980e5c9b43fb7b12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1002698 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ee7aec4715348588310d17bd644c859"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['message', 'message_type', 'message_id', 'conversation_id'],\n        num_rows: 1002698\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# changing the format of the dataset\nds.set_format(type=\"pandas\")\n\n# taking the train split\ndf = ds[\"train\"][:]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:13.619843Z","iopub.execute_input":"2024-06-06T06:33:13.620156Z","iopub.status.idle":"2024-06-06T06:33:15.278268Z","shell.execute_reply.started":"2024-06-06T06:33:13.620131Z","shell.execute_reply":"2024-06-06T06:33:15.277353Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                             message message_type  message_id  \\\n0  What does the \"yield\" keyword do?: What is the...  instruction           0   \n1  To understand what yield does, you must unders...       output           1   \n2  What is a metaclass in Python?: What are metac...  instruction           0   \n3  Classes as objects\\nBefore understanding metac...       output           1   \n4  How to make a chain of function decorators in ...  instruction           0   \n\n   conversation_id  \n0                0  \n1                0  \n2                1  \n3                1  \n4                2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message</th>\n      <th>message_type</th>\n      <th>message_id</th>\n      <th>conversation_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What does the \"yield\" keyword do?: What is the...</td>\n      <td>instruction</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>To understand what yield does, you must unders...</td>\n      <td>output</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is a metaclass in Python?: What are metac...</td>\n      <td>instruction</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Classes as objects\\nBefore understanding metac...</td>\n      <td>output</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How to make a chain of function decorators in ...</td>\n      <td>instruction</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:15.279754Z","iopub.execute_input":"2024-06-06T06:33:15.280530Z","iopub.status.idle":"2024-06-06T06:33:15.286683Z","shell.execute_reply.started":"2024-06-06T06:33:15.280491Z","shell.execute_reply":"2024-06-06T06:33:15.285760Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# df[\"text\"] = df[\"start\"] + \". \" + df[\"code\"] + \". \" + df[\"end\"]\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:15.287944Z","iopub.execute_input":"2024-06-06T06:33:15.288204Z","iopub.status.idle":"2024-06-06T06:33:15.371759Z","shell.execute_reply.started":"2024-06-06T06:33:15.288181Z","shell.execute_reply":"2024-06-06T06:33:15.370903Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"text = \". [EOS] \".join(df[\"message\"].head(100000))","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:15.375175Z","iopub.execute_input":"2024-06-06T06:33:15.375506Z","iopub.status.idle":"2024-06-06T06:33:15.516348Z","shell.execute_reply.started":"2024-06-06T06:33:15.375483Z","shell.execute_reply":"2024-06-06T06:33:15.515338Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# # converting the pandas dataset to hugging face dataset format\n# small_ds = Dataset.from_pandas(df)\n# print(small_ds)\n\n# dataloader = DataLoader(small_ds[\"text\"], batch_size=1, shuffle=True, pin_memory=True)\n# print(len(dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:15.517670Z","iopub.execute_input":"2024-06-06T06:33:15.517986Z","iopub.status.idle":"2024-06-06T06:33:15.527059Z","shell.execute_reply.started":"2024-06-06T06:33:15.517959Z","shell.execute_reply":"2024-06-06T06:33:15.526230Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/working/m/mamba_python-2000_epoch_stage5-3.pt\"\nstate = torch.load(file_path, map_location=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:15.528180Z","iopub.execute_input":"2024-06-06T06:33:15.528714Z","iopub.status.idle":"2024-06-06T06:33:16.685230Z","shell.execute_reply.started":"2024-06-06T06:33:15.528686Z","shell.execute_reply":"2024-06-06T06:33:16.684475Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# loading the tokenizer\ntokenizer = AutoTokenizer.from_pretrained('pt-sk/mamba')\n\n# loading the model from \nmamba_model = Mamba.from_config(\"pt-sk/mamba_python\")\n\n# pipeline to distributed model training\nmodel = fairscale.nn.Pipe(mamba_model(), balance=[15, 12], chunks = 4)\n\n# loading the model\nmodel.load_state_dict(state[\"model_state_dict\"])\n\n# model = fairscale.nn.Pipe(mamba_model(), balance=[15, 12], chunks = 4, devices = [\"xla:0\", \"xla:1\"])\n\n# loading the optimizer\noptimizer = AdamW(model.parameters(), lr=0.00001)\n\n# loading optimizer weights\n# optimizer.load_state_dict(state[\"optimizer_state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:16.686341Z","iopub.execute_input":"2024-06-06T06:33:16.686612Z","iopub.status.idle":"2024-06-06T06:33:21.392676Z","shell.execute_reply.started":"2024-06-06T06:33:16.686589Z","shell.execute_reply":"2024-06-06T06:33:21.391894Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/5.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415b966836b6466fb87183ae71b0551d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/799k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c2ecafb1bc4756a50e321daea19298"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/457k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e15575303144c1397e6b03ea352e4c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f6e1dac03134df2ab9c1cb607a9e52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/563 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1bcd39d0c04d51b7591ea058347dc6"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07603b4e8dec4780bc3415a57dc5f2d4"}},"metadata":{}}]},{"cell_type":"code","source":"optimizer.load_state_dict(state[\"optimizer_state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:21.393678Z","iopub.execute_input":"2024-06-06T06:33:21.394106Z","iopub.status.idle":"2024-06-06T06:33:21.720990Z","shell.execute_reply.started":"2024-06-06T06:33:21.394082Z","shell.execute_reply":"2024-06-06T06:33:21.720202Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokens = tokenizer(text, return_tensors=\"pt\").input_ids.squeeze(0)\n\nblock_size = 512\nbatch_size = 8\n\nlen_tokens = len(tokens)\ndef get_batch():\n    # generate a small batch of data of inputs x and targets y\n    ix = torch.randint(len_tokens - block_size, (batch_size,))\n    x = torch.stack([tokens[i:i+block_size] for i in ix])\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:33:21.722011Z","iopub.execute_input":"2024-06-06T06:33:21.722294Z","iopub.status.idle":"2024-06-06T06:35:21.811513Z","shell.execute_reply.started":"2024-06-06T06:33:21.722259Z","shell.execute_reply":"2024-06-06T06:35:21.810718Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# state = torch.load(\"/kaggle/working/mamba_python/mamba_python_1.pt\", map_location=\"cpu\")\n# mamba_model.load_state_dict(state[\"model_state_dict\"])\n# optimizer.load_state_dict(state[\"optimizer_state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:35:21.812983Z","iopub.execute_input":"2024-06-06T06:35:21.813241Z","iopub.status.idle":"2024-06-06T06:35:21.817092Z","shell.execute_reply.started":"2024-06-06T06:35:21.813219Z","shell.execute_reply":"2024-06-06T06:35:21.815984Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Trainer\nepochs = 2000\niterator = tqdm(range(epochs), desc=\"Training\", postfix={\"train_loss\": 0.0})\n\nfor epoch in iterator:\n  \n  encoded_inp = get_batch()\n  logits = model(encoded_inp.to(\"cuda:0\"))\n\n  labels = encoded_inp.to(\"cuda:1\")\n  \n  shift_logits = logits[:, :-1, :].contiguous()\n  labels = labels[:, 1:].contiguous()\n  loss_fct = torch.nn.CrossEntropyLoss()\n  loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n  \n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\n  \n\n  # moving data's from gpu to cpu\n  loss = loss.to(\"cpu\")\n  logits = logits.to(\"cpu\")\n  labels = labels.to(\"cpu\")\n  encoded_inp = encoded_inp.to(\"cpu\")\n  shift_logits = shift_logits.to(\"cpu\")\n  \n  iterator.set_postfix({\"train_loss\": loss.item()}, refresh=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T06:35:21.818449Z","iopub.execute_input":"2024-06-06T06:35:21.819185Z","iopub.status.idle":"2024-06-06T09:03:51.146311Z","shell.execute_reply.started":"2024-06-06T06:35:21.819148Z","shell.execute_reply":"2024-06-06T09:03:51.145347Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 2000/2000 [2:28:29<00:00,  4.45s/it, train_loss=3.85]  \n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict()\n}, \"mamba_python-standardized100k2000epoch-stage6.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:26.801827Z","iopub.execute_input":"2024-06-06T09:13:26.802205Z","iopub.status.idle":"2024-06-06T09:13:29.346464Z","shell.execute_reply.started":"2024-06-06T09:13:26.802176Z","shell.execute_reply":"2024-06-06T09:13:29.345339Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:56.432508Z","iopub.execute_input":"2024-06-06T09:13:56.432857Z","iopub.status.idle":"2024-06-06T09:13:56.463140Z","shell.execute_reply.started":"2024-06-06T09:13:56.432831Z","shell.execute_reply":"2024-06-06T09:13:56.462274Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14bc553f88d7463aa9e533509630aa7b"}},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/mamba_python-standardized100k2000epoch-stage6.pt\",\n    path_in_repo=\"mamba_python-standardized100k2000epoch-stage6.pt\",\n    repo_id=\"pt-sk/m\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:15:26.736686Z","iopub.execute_input":"2024-06-06T09:15:26.737043Z","iopub.status.idle":"2024-06-06T09:16:17.768781Z","shell.execute_reply.started":"2024-06-06T09:15:26.737014Z","shell.execute_reply":"2024-06-06T09:16:17.767901Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"mamba_python-standardized100k2000epoch-stage6.pt:   0%|          | 0.00/2.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e663d3f2622648b2a457c1fa130a6761"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/pt-sk/m/commit/53de98b25ccfc4a163499cf71f50bd8b6dec9133', commit_message='Upload mamba_python-standardized100k2000epoch-stage6.pt with huggingface_hub', commit_description='', oid='53de98b25ccfc4a163499cf71f50bd8b6dec9133', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# cleaning the gpu memory\nimport gc\n\ndef clear_gpu_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\nif torch.cuda.is_available():\n    clear_gpu_memory()\n    print(\"GPU memory cleared.\")\nelse:\n    print(\"CUDA is not available.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T04:46:01.064285Z","iopub.execute_input":"2024-06-05T04:46:01.064718Z","iopub.status.idle":"2024-06-05T04:46:01.433058Z","shell.execute_reply.started":"2024-06-05T04:46:01.064689Z","shell.execute_reply":"2024-06-05T04:46:01.432153Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"GPU memory cleared.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torch\n# import gc\n\n# def get_gpu_memory_usage():\n#     allocated_memory = torch.cuda.memory_allocated()\n#     reserved_memory = torch.cuda.memory_reserved()\n#     return allocated_memory, reserved_memory\n\n# def list_gpu_variables():\n#     for obj in gc.get_objects():\n#         try:\n#             if torch.is_tensor(obj) and obj.is_cuda:\n#                 print(f\"Tensor on GPU: {obj}, Size: {obj.size()}, Memory: {obj.element_size() * obj.nelement()}\")\n#         except Exception as e:\n#             pass\n\n# if torch.cuda.is_available():\n#     allocated, reserved = get_gpu_memory_usage()\n#     print(f\"Allocated GPU memory: {allocated} bytes\")\n#     print(f\"Reserved GPU memory: {reserved} bytes\")\n#     list_gpu_variables()\n# else:\n#     print(\"CUDA is not available.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:52:13.482929Z","iopub.status.idle":"2024-06-04T18:52:13.483468Z","shell.execute_reply.started":"2024-06-04T18:52:13.483189Z","shell.execute_reply":"2024-06-04T18:52:13.483211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n\ndef generate(model,\n             tokenizer,\n             prompt: str,\n             n_tokens_to_gen: int = 100,\n             sample: bool = True,\n             top_k: int = 40):\n    model.eval()\n\n    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(\"cuda:0\")\n    print(input_ids)\n\n    for token_n in range(n_tokens_to_gen):\n        with torch.no_grad():\n            indices_to_input = input_ids.to(\"cuda:0\")\n            next_token_logits = model(indices_to_input)[:, -1].to(\"cuda:0\")\n\n        probs = F.softmax(next_token_logits, dim=-1)\n        (batch, vocab_size) = probs.shape\n\n        if top_k is not None:\n            (values, indices) = torch.topk(probs, k=top_k)\n            probs[probs < values[:, -1, None]] = 0\n            probs = probs / probs.sum(axis=1, keepdims=True)\n\n        if sample:\n            next_indices = torch.multinomial(probs, num_samples=1)\n        else:\n            next_indices = torch.argmax(probs, dim=-1)[:, None]\n\n        input_ids = torch.cat([input_ids, next_indices], dim=1)\n\n    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n\n    return output_completions","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:22:44.395714Z","iopub.execute_input":"2024-06-06T09:22:44.396073Z","iopub.status.idle":"2024-06-06T09:22:44.406170Z","shell.execute_reply.started":"2024-06-06T09:22:44.396044Z","shell.execute_reply":"2024-06-06T09:22:44.405294Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"generate(model, tokenizer, \"What is a metaclass in Python?\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:26:37.280646Z","iopub.execute_input":"2024-06-06T09:26:37.281008Z","iopub.status.idle":"2024-06-06T09:26:42.617627Z","shell.execute_reply.started":"2024-06-06T09:26:37.280977Z","shell.execute_reply":"2024-06-06T09:26:42.616742Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"tensor([[ 1276,   310,   247,  1313,   317, 14407,   275, 13814,    32]],\n       device='cuda:0')\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'What is a metaclass in Python? I am trying to find an image that doesn\\'t think the way of using the list and the __all__ method and the first is the \"cac\" list containing the __iter__ method. Any you have a dictionary:\\n>>> d = {k: k for k, v in d.items() if v == k.values_count}\\n[1, 2, 3],\\n \\'b\\': [1,2,3,4,5,6,7] else'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}